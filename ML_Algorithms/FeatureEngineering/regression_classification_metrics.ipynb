{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73be57f",
   "metadata": {},
   "source": [
    "\n",
    "# Regression and Classification Metrics\n",
    "\n",
    "This notebook covers various metrics used in regression and classification, including their formulas, explanations, advantages, disadvantages, and Python implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81fb8c3",
   "metadata": {},
   "source": [
    "\n",
    "## Regression Metrics\n",
    "\n",
    "### 1. Mean Absolute Error (MAE)\n",
    "- **Formula:**  \n",
    "  \\[ MAE = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i | \\]\n",
    "- **Python Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56841a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Example data\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "\n",
    "# Compute MAE\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f177f",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Mean Squared Error (MSE)\n",
    "- **Formula:**  \n",
    "  \\[ MSE = \\frac{1}{n} \\sum_{i=1}^{n} ( y_i - \\hat{y}_i )^2 \\]\n",
    "- **Python Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Compute MSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee97ad8",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Root Mean Squared Error (RMSE)\n",
    "- **Formula:**  \n",
    "  \\[ RMSE = \\sqrt{MSE} \\]\n",
    "- **Python Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(\"Root Mean Squared Error:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821762c",
   "metadata": {},
   "source": [
    "\n",
    "### 4. R² Score (Coefficient of Determination)\n",
    "- **Formula:**  \n",
    "  \\[ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} \\]\n",
    "- **Python Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Compute R² Score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"R² Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20f6d21",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Adjusted R² Score\n",
    "- **Formula:**  \n",
    "  \\[ Adjusted R^2 = 1 - \\left( \\frac{(1 - R^2) (n - 1)}{n - k - 1} \\right) \\]\n",
    "  where:\n",
    "  - \\( R^2 \\) is the standard R² score,\n",
    "  - \\( n \\) is the number of observations,\n",
    "  - \\( k \\) is the number of independent variables (features).\n",
    "- **Python Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def adjusted_r2_score(r2, n, k):\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "# Example values\n",
    "n = len(y_true)  # Number of observations\n",
    "k = 1  # Assuming one feature for simplicity\n",
    "adjusted_r2 = adjusted_r2_score(r2, n, k)\n",
    "\n",
    "print(\"Adjusted R² Score:\", adjusted_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2390f4d3",
   "metadata": {},
   "source": [
    "\n",
    "## Classification Metrics\n",
    "\n",
    "### 1. Accuracy\n",
    "- **Formula:**  \n",
    "  \\[ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN} \\]\n",
    "- **Python Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example classification data\n",
    "y_true_class = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
    "y_pred_class = [1, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "# Compute Accuracy\n",
    "accuracy = accuracy_score(y_true_class, y_pred_class)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4c8ee",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Precision, Recall, and F1 Score\n",
    "- **Precision Formula:**  \n",
    "  \\[ Precision = \\frac{TP}{TP + FP} \\]\n",
    "- **Recall Formula:**  \n",
    "  \\[ Recall = \\frac{TP}{TP + FN} \\]\n",
    "- **F1 Score Formula:**  \n",
    "  \\[ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} \\]\n",
    "- **Python Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774df128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute Precision, Recall, and F1 Score\n",
    "precision = precision_score(y_true_class, y_pred_class)\n",
    "recall = recall_score(y_true_class, y_pred_class)\n",
    "f1 = f1_score(y_true_class, y_pred_class)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306df513",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Confusion Matrix\n",
    "- **Formula:** A confusion matrix is represented as:\n",
    "\n",
    "  |               | **Predicted Positive** | **Predicted Negative** |\n",
    "  |--------------|-----------------------|-----------------------|\n",
    "  | **Actual Positive**  | TP (True Positive)  | FN (False Negative)  |\n",
    "  | **Actual Negative**  | FP (False Positive) | TN (True Negative)   |\n",
    "\n",
    "- **Python Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(y_true_class, y_pred_class)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96894ed2",
   "metadata": {},
   "source": [
    "\n",
    "### 4. ROC-AUC Score\n",
    "- **Python Implementation:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a18859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Compute ROC-AUC Score\n",
    "roc_auc = roc_auc_score(y_true_class, y_pred_class)\n",
    "print(\"ROC-AUC Score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91c324",
   "metadata": {},
   "source": [
    "\n",
    "## Type 1 and Type 2 Errors\n",
    "\n",
    "### 1. Type 1 Error (False Positive)\n",
    "- Occurs when we **reject a true null hypothesis** (detecting something that isn’t there).\n",
    "- Example: A COVID test incorrectly says a healthy person has COVID.\n",
    "\n",
    "### 2. Type 2 Error (False Negative)\n",
    "- Occurs when we **fail to reject a false null hypothesis** (missing something that is there).\n",
    "- Example: A cancer test fails to detect cancer in a patient who actually has it.\n",
    "\n",
    "### 3. Python Implementation of Type 1 & Type 2 Errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9533c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Example classification data\n",
    "y_true_class = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
    "y_pred_class = [1, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(y_true_class, y_pred_class)\n",
    "\n",
    "# Extract values\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Calculate Type 1 and Type 2 Error Rates\n",
    "type1_error_rate = FP / (FP + TN)  # False Positive Rate (α)\n",
    "type2_error_rate = FN / (FN + TP)  # False Negative Rate (β)\n",
    "\n",
    "print(\"Type 1 Error Rate (False Positive Rate):\", type1_error_rate)\n",
    "print(\"Type 2 Error Rate (False Negative Rate):\", type2_error_rate)\n",
    "\n",
    "# Visualizing Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "            yticklabels=[\"Actual Negative\", \"Actual Positive\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
